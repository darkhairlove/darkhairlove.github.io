---
layout: single
title:  "작사가_만들기-title"
categories: AI
tag: [python, aiffel, ai]
toc: true
author_profile: false
---

<pre>
데이터 크기: 187088
Examples:
 ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']
</pre>

<pre>
[   2  290   28   94 4486    3    0    0    0    0    0    0    0    0]
[ 290   28   94 4486    3    0    0    0    0    0    0    0    0    0]
</pre>

<pre>
Epoch 1/10
487/487 [==============================] - 37s 38ms/step - loss: 3.2869 - val_loss: 2.8996
Epoch 2/10
487/487 [==============================] - 18s 37ms/step - loss: 2.7434 - val_loss: 2.6463
Epoch 3/10
487/487 [==============================] - 18s 37ms/step - loss: 2.4323 - val_loss: 2.4659
Epoch 4/10
487/487 [==============================] - 18s 37ms/step - loss: 2.1281 - val_loss: 2.3302
Epoch 5/10
487/487 [==============================] - 18s 37ms/step - loss: 1.8379 - val_loss: 2.2290
Epoch 6/10
487/487 [==============================] - 18s 37ms/step - loss: 1.5790 - val_loss: 2.1656
Epoch 7/10
487/487 [==============================] - 18s 37ms/step - loss: 1.3644 - val_loss: 2.1287
Epoch 8/10
487/487 [==============================] - 18s 37ms/step - loss: 1.2008 - val_loss: 2.1240
Epoch 9/10
487/487 [==============================] - 18s 37ms/step - loss: 1.0897 - val_loss: 2.1351
Epoch 10/10
487/487 [==============================] - 18s 37ms/step - loss: 1.0249 - val_loss: 2.1567
</pre>

<pre>
Model: "text_generator"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       multiple                  6144512   
                                                                 
 lstm (LSTM)                 multiple                  20979712  
                                                                 
 lstm_1 (LSTM)               multiple                  33562624  
                                                                 
 dense (Dense)               multiple                  24590049  
                                                                 
=================================================================
Total params: 85,276,897
Trainable params: 85,276,897
Non-trainable params: 0
_________________________________________________________________
</pre>


## 회고

- 어려웠던 점 : embedding_size와 hidden_size를 얼마만큼 늘려야 loss를 줄일지가 어려웠다. model.fit의 인자를 batch_size만 추가했다.

- 알아낸 점 및 모호한 점 : model.fit의 shuffle를 추가한 결과랑 아닌 결과랑 차이점이 있는지 궁금하다. 이미 ataset_train.shuffle()을 통해서 model.fit에서 shuffle을 안해도 되는건지 궁금하다. 출력된 문장에 < start >와 < end >, < unk >는 출력을 안하는 방법이 있는지 궁금하다.

- 노력한 점 :  embedding_size와 hidden_size 외에도 다른 model.fit의 인자를 추가하는 방법을 찾기 위해 노력했다. 토큰화 했을 때, 문장 길이가 15개 이하만 train데이터로 하는 과정을 찾아 넣었다.

- 자기다짐 :  embedding_size와 hidden_size을 여러 가지로 해보는 경험이 중요하다고 느꼈다. train과 test 를 split에서 test data의 비율은 고정이고 radomstate를 바꿨을때 val_loss의 값은 크게 차이가 없다는 것을 보았을 때, embedding과 hidden size가 중요하다는 것을 알 수 있었다. 

